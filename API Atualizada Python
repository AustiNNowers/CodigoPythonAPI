import pandas as pd
from openpyxl import Workbook
from openpyxl.worksheet.table import Table, TableStyleInfo
from openpyxl.utils import get_column_letter
import os
from datetime import datetime
import asyncio
import aiohttp
import time

base_url = "https://prologapp.com/prolog/api/v3/"
end_url = {
            #"checklists" : "checklists?",
            "checklists-work_orders" : "checklists/work-orders?",
            "vehicles" : "vehicles?",
            "vehicles-types" : "vehicles/types/paginated?",
            "vehicles-models" : "vehicles/models/paginated?",
            "vehicles-makes" : "vehicles/makes/paginated?",
            "work_orders" : "work-orders?",
            "users" : "users?"
           }    

token = "qgIfQKWHMoeyQ3sLYGQrr2qV0fx66KgWnIS2FqhfhiuAJ7aZj7d"

headers = {
"x-prolog-api-token" : token,
"Content-Type" : "application/json"
}

unidades = {988: "Transporte", 2083: "Florestal"}
pasta_destino = r"C:\Users\LUIS.BRANCO\OneDrive - Vale do Tibagi\OneDrive - Prolog"
#pasta_destino = "C:\\Users\\LUIS.BRANCO\\Prolog\\dados_diarios"
os.makedirs(pasta_destino, exist_ok=True)

startDate = "2024-12-01"
endDate = datetime.now().date().strftime("%Y-%m-%d")

class Limite_Requisicoes:
    def __init__(self, requisicoes=40, tempo=60):
        self.requisicoes = requisicoes
        self.tempo = tempo
        self.tokens = requisicoes
        self.lock = asyncio.Lock()
        self.ultimo_reset = time.monotonic()

    async def requisicao(self):
        async with self.lock:
            tempo_atual = time.monotonic()
            lapso = tempo_atual - self.ultimo_reset
            self.ultimo_reset = tempo_atual
            self.tokens += lapso * (self.tempo / self.requisicoes)
            if self.tokens > self.requisicoes:
                self.tokens = self.requisicoes
            if self.tokens < 1:
                espere = (1 - self.tokens) * (self.tempo / self.requisicoes)
                await asyncio.sleep(espere)
                self_tokens = 0
            else:
                self.tokens -= 1

limite_requisicoes = Limite_Requisicoes(requisicoes = 40, tempo = 60)

async def gerenciar_loops():
    print("Gerenciando funções...")
    async with aiohttp.ClientSession(headers=headers) as session:

        for nome, endpoint in end_url.items():
            tarefas = []
            tarefas.append(gerenciar_tarefas(endpoint, nome, session))
            await asyncio.gather(*tarefas)

async def gerenciar_tarefas(end_url, nome, session):
    tarefas = []
    for unidade_id, nome_unidade in unidades.items():
        print(f"Unidade: {nome_unidade} - Endpoint: {end_url}")
        tarefas.append(buscar_dados(end_url, unidade_id, nome_unidade, session))

    resultado = await asyncio.gather(*tarefas)
    salvar_dados(resultado, nome, end_url)

async def buscar_dados(endpoint, unidade_id, nome_unidade, session):
    pageNumberBackup = 0
    pageNumber = 0
    todos_dados = []

    print("Entrando no loop...")
    while True:
        params = {}

        if pageNumberBackup != 0:
            pageNumber = pageNumberBackup
            pageNumberBackup = 0

        if endpoint == "checklists?":
            params = {
                "branchOfficesId" : unidade_id,
                "startDate" : startDate,
                "endDate" : endDate,
                "includeAnswers" : "True",
                "pageSize" : 100,
                "pageNumber" : pageNumber
            }
        elif endpoint == "checklists/work-orders?":
            params = {
                "branchOfficesId" : unidade_id,
                "startDate" : startDate,
                "endDate" : endDate,
                "includeAnswers" : "True",
                "pageSize" : 100,
                "pageNumber" : pageNumber
            }
        elif endpoint == "work-orders?":
            params = {
                "branchOfficesId" : unidade_id,
                "includeItems" : "True",
                "pageSize" : 100,
                "pageNumber" : pageNumber
            }
        elif endpoint == "vehicles?":
            params = {
                "branchOfficesId" : unidade_id,
                "includeInactive" : "True",
                "pageSize" : 100,
                "pageNumber" : pageNumber
            }
        elif endpoint == "vehicles/types/paginated?":
            params = {
                "companyId" : 247,
                "pageSize" : 100,
                "pageNumber" : pageNumber
            }
        elif endpoint == "vehicles/models/paginated?":
            params = {
                "companyId" : 247,
                "pageSize" : 100,
                "pageNumber" : pageNumber
            }
        elif endpoint == "vehicles/makes/paginated?":
            params = {
                "companyId" : 247,
                "pageSize" : 100,
                "pageNumber" : pageNumber,
                "includeInactive" : "True",
                "includeInactiveModels" : "True"
            }
        elif endpoint == "users?":
            params = {
                "branchOfficesId" : unidade_id,
                "pageSize" : 100,
                "pageNumber" : pageNumber
            }
        print("escolheu os params: ", endpoint)

        try:
            await limite_requisicoes.requisicao()
            async with session.get(base_url + endpoint, params=params) as resposta:
                if resposta.status == 200:
                    dados = await resposta.json()
                    print("Dados recebidos com sucesso!")
                elif resposta.status == 429:
                    tentar_novamente = int(resposta.headers.get("x-rate-limit-retry-after-seconds", 10))
                    print(f"Rate limit atingido. Aguardando {tentar_novamente} segundos.")
                    await asyncio.sleep(tentar_novamente)
                elif resposta.status in [502, 500]:
                    print(f"Erro {resposta.status}: Bad Gateway. Tentando novamente...")
                    await asyncio.sleep(10)
                    pageNumberBackup = pageNumber
                    continue
                else:
                    print(f"Erro {resposta.status}: {resposta.text}")
                    break

                todos_dados.extend(dados.get("content", []))

                if dados.get("lastPage", True):
                    print(f"Chegou na ultima pagina: {pageNumber}")
                    break

                print("Pagina atual: ", pageNumber)
                pageNumber += 1
                print("Próxima página: ", pageNumber)

        except aiohttp.ClientError as e:
            print(f"Erro de conexão com a API: {e}")
            break
    
    print("Retornando os dados coletados da unidade: ", unidade_id)
    df = pd.DataFrame(todos_dados) if todos_dados else pd.DataFrame()

    if not df.empty:
        if endpoint == "vehicles/types/paginated?" or endpoint == "vehicles/models/paginated?" or endpoint == "vehicles/makes/paginated?":
            return df
        else:
            df["Unidade"] = nome_unidade
            return df
        
    return df

def salvar_dados(planilha, nome, endpoint):
    print("Começando a salvar os dados...")

    df_final = pd.concat(planilha, ignore_index=True)
    nome_arquivo = os.path.join(pasta_destino, f"dados_{nome}.xlsx")

    if endpoint == "checklists?":
        df_final = df_final.explode("formItemsAnswers")

    elif endpoint == "checklists/work-orders?":
        df_final = df_final.explode("workOrderItems")

    elif endpoint == "vehicles?":
        df_final = pd.json_normalize(df_final.to_dict(orient="records"))
        df_final = df_final.explode("coupling.coupledVehicles")

    elif endpoint == "vehicles/makes/paginated?":
        df_final = df_final.explode("models")

    elif endpoint == "work-orders?":
        df_final = pd.json_normalize(df_final.to_dict(orient="records"))
        df_final = df_final.explode("workOrderItems")
        df_final = pd.json_normalize(df_final.to_dict(orient="records"))
        df_final = df_final.explode("workOrderItems.itemServices")
        df_final = df_final.explode("workOrderItems.itemProducts")
        df_final = df_final.explode("workOrderItems.openingAttachments")
        df_final = df_final.explode("workOrderItems.resolutionAttachments")

    df_final = pd.json_normalize(df_final.to_dict(orient="records"))
    print("Planilhas filtradas")
    
    wb = Workbook()
    ws = wb.active
    ws.title = "Prolog"

    ws.append(list(df_final.columns))
    for linha in df_final.itertuples(index=False):
        linha_formatada = [str(celula) if isinstance(celula, (list, dict)) else celula for celula in linha]
        ws.append(linha_formatada)

    n_linhas = df_final.shape[0] + 1
    n_colunas = df_final.shape[1]
    ref_tabela = f"A1:{get_column_letter(n_colunas)}{n_linhas}"

    tabela = Table(displayName="Planilha", ref=ref_tabela)
    estilo = TableStyleInfo(
        name="TableStyleMedium9",
        showFirstColumn=False,
        showLastColumn=False,
        showRowStripes=True,
        showColumnStripes=False
    )
    tabela.tableStyleInfo = estilo
    ws.add_table(tabela)

    for i, coluna in enumerate(ws.columns, 1):
        max_len = max(len(str(cell.value)) if cell.value else 0 for cell in coluna)
        ws.column_dimensions[get_column_letter(i)].width = max_len + 2

    print("Indo salvar a planilha...")
    wb.save(nome_arquivo)
    print(f"Planilha {nome} salva com sucesso!")

if __name__ == "__main__":
    start = time.perf_counter()
    print("Iniciando o processo...")
    asyncio.run(gerenciar_loops())
    print("Processo concluído!")
    end = time.perf_counter()
    print(f"Tempo total: {end - start:.2f} segundos")
